{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rider_detection2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e2cee8b952444acba66cfa387f90a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d63edcc32bdc4ca1b36a3256cda8d66e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7b9eb3fa5b254373b6153638c927d5a1",
              "IPY_MODEL_7dfac1f50eda4ecaad844fd119a28b2a"
            ]
          }
        },
        "d63edcc32bdc4ca1b36a3256cda8d66e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b9eb3fa5b254373b6153638c927d5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ed13928a7694883a7a1afbe3dbae8f9",
            "_dom_classes": [],
            "description": "extract frames",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 527,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 527,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd379d22785f4fb0a144364b7f03cae7"
          }
        },
        "7dfac1f50eda4ecaad844fd119a28b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_80d87de1d6d34c2ba82ef60606a4cf34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 527/527 [04:05&lt;00:00,  2.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b2c7cbc402047ca89693134af7d2139"
          }
        },
        "9ed13928a7694883a7a1afbe3dbae8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd379d22785f4fb0a144364b7f03cae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80d87de1d6d34c2ba82ef60606a4cf34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b2c7cbc402047ca89693134af7d2139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TabeaPreusser/Horse_Detection/blob/master/rider_detection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H770ntdEhav7",
        "colab_type": "code",
        "outputId": "b2557a2b-880e-4c80-b3bc-ed815263bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 111.83 MiB | 12.40 MiB/s, done.\n",
            "Resolving deltas: 100% (569/569), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6j9lcbDk37D",
        "colab_type": "code",
        "outputId": "3fd1dd4c-4726-495c-b112-dde2d587124e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "os.chdir('./Mask_RCNN')\n",
        "!ls\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\tLICENSE      mrcnn\trequirements.txt  setup.cfg\n",
            "images\tMANIFEST.in  README.md\tsamples\t\t  setup.py\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.6.egg\n",
            "Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n",
            "Name: mask-rcnn\n",
            "Version: 2.1\n",
            "Summary: Mask R-CNN for object detection and instance segmentation\n",
            "Home-page: https://github.com/matterport/Mask_RCNN\n",
            "Author: Matterport\n",
            "Author-email: waleed.abdulla@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Requires: \n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53NEtGYfnhx8",
        "colab_type": "code",
        "outputId": "f74004d3-8a22-4e28-ddcb-d29c0048fdf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpQqY7l_946i",
        "colab_type": "text"
      },
      "source": [
        "New Version with seperate classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZgOS-tbpHPp",
        "colab_type": "text"
      },
      "source": [
        "Rider Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yMePuH3Mwjjc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "2575c79d-e825-4b69-8d66-edafaa7cca6a"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import cv2\n",
        "import moviepy.editor as mpe\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "\n",
        "import math\n",
        "from numba import jit\n",
        "from tqdm import tnrange\n",
        "\n",
        "# class that defines and loads the dataset for train and test\n",
        "class RiderDataset(Dataset):\n",
        "   \n",
        "    def getClassname(self,index):\n",
        "        class_names = ['BG', 'horse', 'rider']\n",
        "        if index>=0 and index<3 :\n",
        "          return class_names[i]\n",
        "        else:\n",
        "          return -1\n",
        "    def load_dataset(self, dataset_path, is_train=True):\n",
        "        # define classes\n",
        "        self.add_class(\"dataset\", 1, \"horse\")\n",
        "        self.add_class(\"dataset\", 2, \"rider\")\n",
        "\n",
        "        FRAME_SOURCE = \"./Horse_Detection/rimondo_frames\"\n",
        "        # where to find the labels\n",
        "        db = \"./Horse_Detection/annotations\"\n",
        "\n",
        "        videos = [d for d in os.listdir(FRAME_SOURCE) if os.path.isdir(join(FRAME_SOURCE, d))]\n",
        "        counter = 0\n",
        "        for vid in videos:\n",
        "            video_path = join(FRAME_SOURCE, vid)\n",
        "            frames = [f for f in os.listdir(video_path) if f.endswith(\".png\")]\n",
        "            numberDataset = len(frames)\n",
        "            borderTest = round(numberDataset * 0.9)\n",
        "\n",
        "            for frame in frames:\n",
        "                counter = counter + 1\n",
        "\n",
        "                image_path = FRAME_SOURCE + \"/\" + vid + \"/\" + frame\n",
        "                image_id = vid + \"/\" + frame\n",
        "                # skip all test images if we are building the train set\n",
        "                if is_train and counter <= borderTest:\n",
        "                    continue\n",
        "                # skip all train images if we are building the test/val set\n",
        "                if not is_train and counter > borderTest:\n",
        "                    continue\n",
        "                # add to dataset\n",
        "                self.add_image('dataset', image_id=counter, path=image_path, annotation=db)\n",
        "\n",
        "    def extract_boxes(self, img_path, db):\n",
        "        fileName, fileExtension = os.path.splitext(img_path)\n",
        "        fileName = join(fileName, '.jpg')\n",
        "        # get image name for search in database\n",
        "        path_list = fileName.split(os.sep)\n",
        "        imageNameDatabase = path_list[-3] + '/' + path_list[-2] + path_list[-1]\n",
        "        db_path = join(db, imageNameDatabase)\n",
        "        db_path = os.path.splitext(db_path)[0] + '.csv'\n",
        "        db_path = os.path.normpath(db_path)\n",
        "        label_data = pd.read_csv(db_path)\n",
        "        allEntries = label_data.loc[(label_data[\"image\"] == imageNameDatabase)]\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        height = img.shape[0]\n",
        "        width = img.shape[1]\n",
        "        bbox = []\n",
        "        for _, entry in allEntries.iterrows():\n",
        "\n",
        "            x = entry[\"x\"] * width\n",
        "            y = entry[\"y\"] * height\n",
        "            w = entry[\"width\"] * width\n",
        "            h = entry[\"height\"] * height\n",
        "            # Eckpunkte bbox berechnen\n",
        "            xmin = round(x - w // 2)\n",
        "            xmax = round(x + w // 2)\n",
        "            ymin = round(y - h // 2)\n",
        "            ymax = round(y + h // 2)\n",
        "            if (entry[\"label\"] == 1 or entry[\"label\"] == 2):\n",
        "                if (math.isnan(entry[\"x\"]) or math.isnan(entry[\"y\"]) or math.isnan(entry[\"width\"]) or math.isnan(\n",
        "                        entry[\"height\"])):\n",
        "                    continue\n",
        "                box = [entry[\"label\"], xmin, ymin, xmax, ymax]\n",
        "                bbox.append(box)\n",
        "        return bbox, height, width\n",
        "\n",
        "    # load the masks for an image\n",
        "    def load_mask(self, image_id):\n",
        "        # get details of image\n",
        "        info = self.image_info[image_id]\n",
        "        # define box file location\n",
        "        db_path = info['annotation']\n",
        "        img_path = info['path']\n",
        "\n",
        "        # load boxes\n",
        "        boxes, h, w = self.extract_boxes(img_path, db_path)\n",
        "        # create one array for all masks, each on a different channel\n",
        "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
        "        # create masks\n",
        "        class_ids = list()\n",
        "        for i in range(len(boxes)):\n",
        "            box = boxes[i]\n",
        "            row_s, row_e = box[2], box[4]\n",
        "            col_s, col_e = box[1], box[3]\n",
        "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
        "\n",
        "            if (box[0] == 1):\n",
        "                class_ids.append(self.class_names.index('horse'))\n",
        "            if (box[0] == 2):\n",
        "                class_ids.append(self.class_names.index('rider'))\n",
        "        return masks, asarray(class_ids, dtype='int32')\n",
        "\n",
        "    # load an image reference\n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        return info['path']\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1400832/45929032 bytes (3.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5701632/45929032 bytes (12.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10149888/45929032 bytes (22.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14483456/45929032 bytes (31.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18735104/45929032 bytes (40.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23019520/45929032 bytes (50.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27312128/45929032 bytes (59.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31662080/45929032 bytes (68.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36085760/45929032 bytes (78.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40476672/45929032 bytes (88.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44908544/45929032 bytes (97.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ3CYoUiwubw",
        "colab_type": "text"
      },
      "source": [
        "Configuration Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsNBSXbDwt10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the prediction configuration\n",
        "class RiderConfig(Config):\n",
        "    # define the name of the configuration\n",
        "    NAME = \"rider_cfg\"\n",
        "    # number of classes (background + rider+horse)\n",
        "    NUM_CLASSES = 1 + 2\n",
        "    # simplify GPU config\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    #\n",
        "    STEPS_PER_EPOCH = 1000\n",
        "    VALIDATION_STEPS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv2sPqrS1MLI",
        "colab_type": "text"
      },
      "source": [
        "Detection Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzyUiW_Y1Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detection of rider by bounding boxes\n",
        "class rider_detection:\n",
        "\n",
        "  def __init__(self, path_weights):\n",
        "    self.model=self.setModel(path_weights)\n",
        "\n",
        "  def setModel(self,path_weights):\n",
        "        config = RiderConfig()\n",
        "        # define the model\n",
        "        model = MaskRCNN(mode='inference', model_dir='./', config=config)\n",
        "        # load trained weights \n",
        "        model.load_weights(path_weights, by_name=True)\n",
        "        return model\n",
        "\n",
        "  # get bounding boxes for one image\n",
        "  def getBboxes(self,im):\n",
        "\t  results = self.model.detect([im], verbose=0)\n",
        "\t  r = results[0]\n",
        "\t  # extract ids and bboxes for riders\n",
        "\t  rois = r['rois']\n",
        "\n",
        "\t  bbox = []\n",
        "\t  counter=-1\n",
        "\t  for i in r['class_ids']:\n",
        "\t\t  counter=counter+1\n",
        "\t\t  b=self.box_to_dict(rois[counter])\n",
        "\t\t  bbox.append(b)\n",
        "  \t  # austauschen mit crop methode statt zeichnen\n",
        "\t    #drawBox(imagepath, bbox)\n",
        "\t  return bbox\n",
        "  \n",
        "  # get bounding boxes for list of frames\n",
        "  def getBboxes_framelist(self,frames):\n",
        "    bbox_list=list()\n",
        "    for i in range(len(frames)):\n",
        "      bbox_list.append(self.getBboxes(frames[i]))\n",
        "    return bbox_list\n",
        "\n",
        "  def box_to_dict(self,box):\n",
        "\t    b={\"x1\":box[1],\"x2\":box[3],\"y1\":box[0],\"y2\":box[2]}\n",
        "\t    return b\n",
        "  \n",
        "  def upscale_bbox(self,bbox):\n",
        "    x1=bbox[\"x1\"]*2\n",
        "    x2=bbox[\"x2\"]*2\n",
        "    y1=bbox[\"y1\"]*2\n",
        "    y2=bbox[\"y2\"]*2\n",
        "    b={\"x1\":x1,\"x2\":x2,\"y1\":y1,\"y2\":y2}\n",
        "    return b\n",
        "  \n",
        "  def upscale_all_bboxes(self,bbox_list):\n",
        "    upscaled_bboxes=list()\n",
        "    for i in range(len(bbox_list)):\n",
        "      boxes=list()\n",
        "      for j in bbox_list[i]:\n",
        "        boxes.append(self.upscale_bbox(j))\n",
        "      upscaled_bboxes.append(boxes)\n",
        "      del boxes\n",
        "    return upscaled_bboxes\n",
        "\n",
        "  def upscale_bboxes_list(self,bbox_list):\n",
        "    upscaled_bboxes=list()\n",
        "    for i in bbox_list:\n",
        "      upscaled_bboxes.append(self.upscale_bbox(i))\n",
        "    return upscaled_bboxes\n",
        "\n",
        "\n",
        "  # only for debugging\n",
        "  def drawBox(self,image, bbox):\n",
        "    im = cv2.imread(image)\n",
        "    for b in bbox:\n",
        "        y1 = b[\"y1\"]\n",
        "        x1 = b[\"x1\"]\n",
        "        y2 = b[\"y2\"]\n",
        "        x2 = b[\"x2\"]\n",
        "\n",
        "        im[y1:y2, x1:x1 + 5] = (0, 0, 0)\n",
        "        im[y1:y2, x2:x2 + 5] = (0, 0, 0)\n",
        "        im[y1:y1 + 5, x1:x2] = (0, 0, 0)\n",
        "        im[y2:y2 + 5, x1:x2] = (0, 0, 0)\n",
        "    im = cv2.resize(im, None, fx=0.3, fy=0.3)\n",
        "    cv2.imshow('title', im)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1WMDhWUw2Z9",
        "colab_type": "text"
      },
      "source": [
        "Cropping Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm6qW5Y6xLsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cutting of frames according to the rider\n",
        "class Cropping:\n",
        "    def __init__(self):\n",
        "      pass\n",
        " \n",
        "      # crop one frame\n",
        "    def complete_bbox(self, frame,bboxes):\n",
        "        sizeX = 1280  # HD\n",
        "        sizeY = 720\n",
        "        size = (sizeX, sizeY)\n",
        "        ratio = sizeX / sizeY\n",
        "\n",
        "      \n",
        "        box=self.filter_boxes(frame,bboxes)\n",
        "\n",
        "        box=self.calculate_box(box,sizeX,sizeY,frame)\n",
        "    \n",
        "        return box\n",
        "   \n",
        "    # create big bounding box by using max and min values for edges\n",
        "    def filter_boxes(self, frame,box_list):\n",
        "\n",
        "      # collect all x, y values \n",
        "      X1arr = []\n",
        "      Y1arr = []\n",
        "      X2arr = []\n",
        "      Y2arr = []\n",
        "              \n",
        "      for j in box_list:\n",
        "        X1arr.append(j[\"x1\"])\n",
        "        Y1arr.append(j[\"y1\"])\n",
        "        X2arr.append(j[\"x2\"])\n",
        "        Y2arr.append(j[\"y2\"])\n",
        "\n",
        "      # case no boxes are found    \n",
        "      if(len(X1arr)==0):\n",
        "        calculated_box={\"x1\":-1,\"x2\":-1,\"y1\":-1,\"y2\":-1,\"frame\":frame}\n",
        "      else:\n",
        "        X1 = min(X1arr)\n",
        "        Y1 = min(Y1arr)\n",
        "        X2 = max(X2arr)\n",
        "        Y2 = max(Y2arr)\n",
        "        calculated_box={\"x1\":X1,\"x2\":X2,\"y1\":Y1,\"y2\":Y2,\"frame\":frame}\n",
        "      \n",
        "      return calculated_box\n",
        "\n",
        "    # consider borders and ratio of frames for cropping\n",
        "    def calculate_box(self,box,sizeX,sizeY,img):\n",
        "      X1 = box[\"x1\"]\n",
        "      Y1 = box[\"y1\"]\n",
        "      X2 = box[\"x2\"]\n",
        "      Y2 = box[\"y2\"]\n",
        "\n",
        "      TotalYPixels = Y2 - Y1  # Total amount of pixels of the bounding box in Y direction\n",
        "      TotalXPixels=X2-X1\n",
        "      TotalYPixels=1.2*TotalYPixels\n",
        "      TotalXPixels=1.2*TotalXPixels\n",
        "\n",
        "      if(TotalXPixels>TotalYPixels):\n",
        "        if TotalXPixels < 480: TotalXPixels = 480\n",
        "        XRatio=sizeX/TotalXPixels\n",
        "        YRatio=sizeY/XRatio\n",
        "        TotalYPixels = YRatio  # Total amount of pixel of the bounding box in X direction\n",
        "      else:\n",
        "        if TotalYPixels < 480: TotalYPixels = 480\n",
        "        YRatio=sizeY/TotalYPixels\n",
        "        XRatio=sizeX/YRatio\n",
        "        TotalXPixels = XRatio  # Total amount of pixel of the bounding box in X direction\n",
        "                \n",
        "      # Calculate optimal Crop points to maintain ratio\n",
        "      CropX1 = ((X2 + X1) / 2) - (TotalXPixels / 2)  # Defines the left most pos to crop the pic\n",
        "      CropX2 = ((X2 + X1) / 2) + (TotalXPixels / 2)  # Defines the right most pos to crop the pic\n",
        "      CropY1 = ((Y2 + Y1) / 2) - (TotalYPixels / 2)  # Defines the top most pos to crop the pic\n",
        "      CropY2 = ((Y2 + Y1) / 2) + (TotalYPixels / 2)  # Defines the bottom most pos to crop the pic\n",
        "                \n",
        "      # Out of Bounds check\n",
        "      if CropX1 < 0:  # If we are out of bounds to the left side, adjust both X pos to the right\n",
        "        CropX2 += -1 * CropX1\n",
        "        CropX1 = 0\n",
        "\n",
        "      if CropX2 > img.shape[1]:  # If we are out of bounds to the right side, adjust both X pos to the left\n",
        "        CropX1 += (img.shape[1] - CropX2)\n",
        "        CropX2 = img.shape[1]\n",
        "\n",
        "      if CropY1 < 0:  # If we are out of bounds to the upper side, adjust both Y pos down\n",
        "        CropY2 += -1 * CropY1\n",
        "        CropY1 = 0\n",
        "\n",
        "      if CropY2 > img.shape[0]:  # If we are out of bounds to the lower side, adjust both Y pos up\n",
        "        CropY1 += (img.shape[0] - CropY2)\n",
        "        CropY2 = img.shape[0]\n",
        "\n",
        "                \n",
        "                \n",
        "      CropX1=int(CropX1)\n",
        "      CropX2=int(CropX2)\n",
        "      CropY1=int(CropY1)\n",
        "      CropY2=int(CropY2)\n",
        "      box=self.make_bbox( CropX1,CropX2,CropY1,CropY2)\n",
        "      return box\n",
        "\n",
        "    # crop picture by x,y values\n",
        "    def crop_image(self,box,img):\n",
        "      X1 = box[\"x1\"]\n",
        "      Y1 = box[\"y1\"]\n",
        "      X2 = box[\"x2\"]\n",
        "      Y2 = box[\"y2\"]\n",
        "      if X1 != img.shape[1] and Y1 != img.shape[0]:\n",
        "          img = img[Y1:Y2,X1:X2]\n",
        "      return img\n",
        "    \n",
        "    def make_bbox(self, x1,x2,y1,y2):\n",
        "      b={\"x1\":x1,\"x2\":x2,\"y1\":y1,\"y2\":y2}\n",
        "      return b\n",
        "\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tPfTOWb5RIc",
        "colab_type": "text"
      },
      "source": [
        "Video quality class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQn4qpNg5fVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import queue\n",
        "class VideoQuality:\n",
        "  # get last bbox if none found in current frame\n",
        "  def fill_missing_bboxes(self,bboxes):\n",
        "    for i in range(len(bboxes)):\n",
        "      if(len(bboxes[i])==0 and i>0):\n",
        "        bboxes[i]=bboxes[i-1]\n",
        "    return bboxes\n",
        "\n",
        "  # calculate bbox by last 2 frames\n",
        "  def calculate_missing_bbox_last_frames(self,bboxes):\n",
        "    if bboxes.empty():\n",
        "      return self.make_bbox(-1,-1,-1,-1)\n",
        "    if bboxes.qsize()==1:\n",
        "      return bboxes.get()\n",
        "    while bboxes.qsize()>2:\n",
        "      bboxes.get()\n",
        "    # bboxes from last 2 frames\n",
        "    second=bboxes.get()\n",
        "    first=bboxes.get()\n",
        "\n",
        "    box=self.mean_boxes(first,second)\n",
        "    return box\n",
        "\n",
        "  def mean_boxes(self, first, second):\n",
        "    x1=int((first[\"x1\"]+second[\"x1\"])/2)\n",
        "    x2=int((first[\"x2\"]+second[\"x2\"])/2)\n",
        "    y1=int((first[\"y1\"]+second[\"y1\"])/2)\n",
        "    y2=int((first[\"y2\"]+second[\"y2\"])/2)\n",
        "    return make_bbox(x1,x2,y1,y2)\n",
        "\n",
        "  def make_bbox(self, x1,x2,y1,y2):\n",
        "      b={\"x1\":x1,\"x2\":x2,\"y1\":y1,\"y2\":y2}\n",
        "      return b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESvveTCZ5cIj",
        "colab_type": "text"
      },
      "source": [
        "Video Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KBfuaq45hKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import queue\n",
        "\n",
        "class VideoProcessing:\n",
        "\n",
        "    def __init__(self,path_weights):\n",
        "      self.detector=rider_detection(path_weights)\n",
        "      self.cropper=Cropping()\n",
        "      self.vidQuality=VideoQuality()\n",
        "\n",
        "    def testVideo(self,path):\n",
        "      \n",
        "      cap = cv2.VideoCapture(path)\n",
        "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "      sizeX = 640  \n",
        "      sizeY = 480\n",
        "      size = (sizeX, sizeY)\n",
        "      ratio = sizeX / sizeY\n",
        "\n",
        "      # Define the codec and create VideoWriter object\n",
        "      fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "      out = cv2.VideoWriter('/content/drive/My Drive/CV Praktikum/Test.mp4',fourcc, fps, size)\n",
        "\n",
        "      while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if ret==True:\n",
        "         \n",
        "          frame=cv2.resize(frame,size)\n",
        "\n",
        "          # write the flipped frame\n",
        "          out.write(frame)\n",
        "        else:\n",
        "          break\n",
        "\n",
        "    \t# Release everything if job is finished\n",
        "      cap.release()\n",
        "      out.release()\n",
        "    \n",
        "    def process_frames(self, video_path: str):\n",
        "        video = cv2.VideoCapture(video_path)\n",
        "        filename = os.path.basename(video_path)\n",
        "        total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "        print(total_frames)\n",
        "\n",
        "        fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        # Check for first picture\n",
        "        success, img = video.read()\n",
        "\n",
        "        sizeX = 800  # HD\n",
        "        sizeY = 450\n",
        "        size = (sizeX, sizeY)\n",
        "        ratio = sizeX / sizeY\n",
        "         \n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        out = cv2.VideoWriter('/content/drive/My Drive/CV Praktikum/Cropping.mp4', fourcc, fps, size)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        last_frames=queue.Queue(10)\n",
        "        last_bboxes=queue.Queue(10)\n",
        "        for i in tnrange(int(total_frames),desc='extract frames'):\n",
        "            \n",
        "            if success:\n",
        "              # save last 10 images in queue\n",
        "              if last_frames.qsize()==10:\n",
        "                last_frames.get()\n",
        "              last_frames.put(img)\n",
        "\n",
        "              #downsize frame and detect\n",
        "              #downsize_img=self.downscale_frame(img,0.5)\n",
        "              bboxes=self.detector.getBboxes(img)\n",
        "\n",
        "              # calculate missing bboxes and scale them up \n",
        "              #bboxes=self.detector.upscale_bboxes_list(bboxes)\n",
        "\n",
        "              # crop frame\n",
        "              box=self.cropper.complete_bbox(img,bboxes)\n",
        "              \n",
        "              if(len(bboxes)==0):\n",
        "                box=self.vidQuality.calculate_missing_bbox_last_frames(last_bboxes)\n",
        "              \n",
        "                \n",
        "              cropped_frame=self.cropper.crop_image(box,img)\n",
        "\n",
        "              # save calculated box for frame\n",
        "              if last_bboxes.qsize()==10:\n",
        "                last_bboxes.get()\n",
        "              last_bboxes.put(box)\n",
        "\n",
        "              # resize picture to output size\n",
        "              img = cv2.resize(cropped_frame, size)\n",
        "              out.write(img)\n",
        "            del img\n",
        "            success, img = video.read()\n",
        "        video.release();\n",
        "        out.release()\n",
        "        \n",
        "\n",
        "    def downscale_frames(self,frames):\n",
        "      small_frames=list()\n",
        "      for i in tnrange(len(frames),desc='downsize frames'):\n",
        "        small_frames.append(cv2.resize(frames[i], (0,0), fx=0.5, fy=0.5) )\n",
        "      return small_frames\n",
        "\n",
        "    def downscale_frame(self,frame,scale):\n",
        "      return cv2.resize(frame, (0,0), fx=scale, fy=scale)\n",
        "    \n",
        "    \n",
        "\n",
        "        #videoclip = mpe.VideoFileClip(filename.strip('.MP4') + 'Crop.mp4')\n",
        "        # Audiostuff, takes alot longer tho\n",
        "        # audioclip = mpe.CompositeAudioClip([mpe.AudioFileClip(video_path)])\n",
        "        # videoclip.audio = audioclip\n",
        "        # videoclip.write_videofile(filename.strip('.mp4')+'Crop.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3G-boS7LJMd",
        "colab_type": "code",
        "outputId": "cc4fe857-124b-4123-fdd4-c1943a0f244a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0e2cee8b952444acba66cfa387f90a10",
            "d63edcc32bdc4ca1b36a3256cda8d66e",
            "7b9eb3fa5b254373b6153638c927d5a1",
            "7dfac1f50eda4ecaad844fd119a28b2a",
            "9ed13928a7694883a7a1afbe3dbae8f9",
            "fd379d22785f4fb0a144364b7f03cae7",
            "80d87de1d6d34c2ba82ef60606a4cf34",
            "8b2c7cbc402047ca89693134af7d2139"
          ]
        }
      },
      "source": [
        "v=VideoProcessing('/content/drive/My Drive/CV Praktikum/mask_rcnn_rider_cfg_0006.h5')\n",
        "v.process_frames('/content/drive/My Drive/CV Praktikum/ZOOM0004_1.mp4')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "527.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e2cee8b952444acba66cfa387f90a10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='extract frames', max=527, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1rFtPKhw4PL",
        "colab_type": "text"
      },
      "source": [
        "Operator Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwR9mUXKw568",
        "colab_type": "text"
      },
      "source": [
        "Main: Detection test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SKHocgDKlyr",
        "colab_type": "text"
      },
      "source": [
        "Verbesserungen:\n",
        "\n",
        "\n",
        "*   nicht in jedem frame detektieren\n",
        "*   videoqualität verbessern (smoother)\n",
        "*   bei fehlenden bboxen mittelwerte nuten\n",
        "*   beim filtern nur ein Pferd detektieren\n",
        "*   mehr Reiter/Pferde trainieren\n",
        "*   Test mit Publikum\n",
        "*   Reiterpaar detektieren\n",
        "*  lineare regression für fehlende bboxen\n",
        "*  Spiegel\n",
        "*  Kalmanfilter\n",
        "*  Visualisierung von BBoxen\n",
        "\n",
        "Performance:\n",
        "\n",
        "\n",
        "*   resize (teilweise integriert)\n",
        "*   weniger frames detektieren\n",
        "*   frame Listen nicht zwischenspeichern/ kleinere Teile damit nicht so viel Ram benötigt wird\n",
        "\n",
        "\n",
        "Idee:\n",
        "evtl. die frames nicht zwischenspeichern sondern einzeln bearbeiten, dabei die letzen 10 in einer queue zwischenspeichern für Berechnungen\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}